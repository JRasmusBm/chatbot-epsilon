{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/JRasmusBm/chatbot-\n",
    "epsilon/blob/master/Trainer.ipynb\" target=\"_parent\"><img\n",
    "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In\n",
    "Colab\"/></a>\n",
    "\n",
    "# This is the file in which we perform training of the NN\n",
    "\n",
    "# Load\n",
    "Data\n",
    "\n",
    "## In Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "63"
    }
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n",
    "#file_name = \"amazon_cells_labelled.txt\"\n",
    "#uploaded[file_name].decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "64"
    }
   },
   "outputs": [],
   "source": [
    "data_folder = \"../../data\"\n",
    "file_name = f\"{data_folder}/amazon_cells_labelled.txt\"\n",
    "json_file = f\"{data_folder}/amazon_cells_labelled.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import code (from TA)\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "83"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data\n",
    "\n",
    "First, we create lists of labels and sentences. The indices in\n",
    "the one\n",
    "correspond to those in the other. Due to restrictions in torchtext,\n",
    "write it as\n",
    "json to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "66"
    }
   },
   "outputs": [],
   "source": [
    "with open(file_name) as f:\n",
    "    contents = f.read()\n",
    "labels = []\n",
    "sentences = []\n",
    "for line in (l for l in contents.split(\"\\n\") if l):\n",
    "    labels.append(int(line[-1]))\n",
    "    sentence = str.strip(line[:-1])\n",
    "    while len(sentence.split(\" \")) < 5:\n",
    "        sentence += \" a\"\n",
    "    sentences.append(sentence)\n",
    "data_json = [\n",
    "    dict(label=label, text=text) for label, text in zip(labels, sentences)\n",
    "]\n",
    "with open(json_file, \"w\") as f:\n",
    "    text = \"\\n\".join(json.dumps(line) for line in data_json)\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "67"
    }
   },
   "outputs": [],
   "source": [
    "with open(json_file) as f:\n",
    "    json_written = [json.loads(line) for line in f.read().split(\"\\n\")]\n",
    "    for line in json_written:\n",
    "        if line[\"label\"] not in [0, 1]:\n",
    "            print(line)\n",
    "        if len(line[\"text\"].split(\" \")) < 5:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Torchtext Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(\" \".join(n_gram))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "68"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize=\"spacy\", batch_first=True,)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "fields = dict(text=(\"text\", TEXT), label=(\"label\", LABEL),)\n",
    "\n",
    "dataset = data.TabularDataset(path=json_file, format=\"json\", fields=fields,)\n",
    "# help(dataset)\n",
    "training_data, test_data, validation_data = dataset.split(\n",
    "    split_ratio=[0.7, 0.2, 0.1], random_state=random.seed(SEED)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "69"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length (Training Data): 700\n",
      "Length (Test Data): 100\n",
      "Length (Validation Data): 200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length (Training Data): {len(training_data)}\")\n",
    "print(f\"Length (Test Data): {len(test_data)}\")\n",
    "print(f\"Length (Validation Data): {len(validation_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "70"
    }
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(\n",
    "  training_data,\n",
    "  vectors=\"glove.6B.100d\",\n",
    "  unk_init=torch.Tensor.normal_,\n",
    "  max_size = MAX_VOCAB_SIZE\n",
    ")\n",
    "LABEL.build_vocab(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "71"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 1828\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "[('the', 300), ('and', 201), ('I', 195), ('is', 175), ('a', 143), ('it', 141), ('to', 136), ('phone', 105), ('this', 100), ('my', 86)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "print(TEXT.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "75"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "training_iterator, validation_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (training_data, validation_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "77"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        n_filters,\n",
    "        filter_sizes,\n",
    "        output_dim,\n",
    "        dropout,\n",
    "        pad_idx,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=pad_idx\n",
    "        )\n",
    "        self.conv_0 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=n_filters,\n",
    "            kernel_size=(filter_sizes[0], embedding_dim),\n",
    "        )\n",
    "        self.conv_1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=n_filters,\n",
    "            kernel_size=(filter_sizes[1], embedding_dim),\n",
    "        )\n",
    "        self.conv_2 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=n_filters,\n",
    "            kernel_size=(filter_sizes[2], embedding_dim),\n",
    "        )\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        conved_0 = nn.functional.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = nn.functional.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = nn.functional.relu(self.conv_2(embedded).squeeze(3))\n",
    "        pooled_0 = nn.functional.max_pool1d(\n",
    "            conved_0, conved_0.shape[2]\n",
    "        ).squeeze(2)\n",
    "        pooled_1 = nn.functional.max_pool1d(\n",
    "            conved_1, conved_1.shape[2]\n",
    "        ).squeeze(2)\n",
    "        pooled_2 = nn.functional.max_pool1d(\n",
    "            conved_2, conved_2.shape[2]\n",
    "        ).squeeze(2)\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "78"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PADDING_INDEX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(\n",
    "    INPUT_DIM,\n",
    "    EMBEDDING_DIM,\n",
    "    N_FILTERS,\n",
    "    FILTER_SIZES,\n",
    "    OUTPUT_DIM,\n",
    "    DROPOUT,\n",
    "    PADDING_INDEX,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "79"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 274,705 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_INDEX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNKNOWN_INDEX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PADDING_INDEX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "81"
    }
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "80"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "82"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "84"
    }
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    training_loss, training_acc = train(model, training_iterator, optimizer, criterion)\n",
    "    validation_loss, validation_acc = evaluate(model, validation_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\ttraining Loss: {training_loss:.3f} | training Acc: {training_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {validation_loss:.3f} |  Val. Acc: {validation_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
