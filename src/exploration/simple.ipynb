{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/JRasmusBm/chatbot-\n",
    "epsilon/blob/master/Trainer.ipynb\" target=\"_parent\"><img\n",
    "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In\n",
    "Colab\"/></a>\n",
    "\n",
    "# This is the file in which we perform training of the NN\n",
    "\n",
    "# Load\n",
    "Data\n",
    "\n",
    "## In Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "63"
    }
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n",
    "#file_name = \"amazon_cells_labelled.txt\"\n",
    "#uploaded[file_name].decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "64"
    }
   },
   "outputs": [],
   "source": [
    "data_folder = \"../../data\"\n",
    "file_name = f\"{data_folder}/amazon_cells_labelled.txt\"\n",
    "json_file = f\"{data_folder}/amazon_cells_labelled.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import code (from TA)\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "83"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import torch\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data\n",
    "\n",
    "First, we create lists of labels and sentences. The indices in\n",
    "the one\n",
    "correspond to those in the other. Due to restrictions in torchtext,\n",
    "write it as\n",
    "json to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "66"
    }
   },
   "outputs": [],
   "source": [
    "with open(file_name) as f:\n",
    "    contents = f.read()\n",
    "labels = []\n",
    "sentences = []\n",
    "for line in (l for l in contents.split(\"\\n\") if l):\n",
    "    labels.append(int(line[-1]))\n",
    "    sentences.append(str.strip(line[:-1]).replace(\n",
    "        \".\", \"\").replace(\n",
    "        \"!\", \"\").replace(\n",
    "        \"?\", \"\").replace(\n",
    "        \",\", \"\").split(\" \"))\n",
    "data_json = [\n",
    "    dict(label=label, text=text)\n",
    "    for label, text in zip(labels, sentences)\n",
    "]\n",
    "with open(json_file, \"w\") as f:\n",
    "    text = \"\\n\".join(json.dumps(line) for line in data_json)\n",
    "    f.write(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "67"
    }
   },
   "outputs": [],
   "source": [
    "with open(json_file) as f:\n",
    "    json_written = [json.loads(line) for line in f.read().split(\"\\n\")]\n",
    "    for line in json_written:\n",
    "        if line[\"label\"] not in [0, 1]:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the lists of labels into a torchtext dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "68"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data.dataset import random_split\n",
    "SEED = 1234\n",
    "\n",
    "\n",
    "TEXT = data.Field(sequential=True)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "fields=dict(\n",
    "    text=(\"text\", TEXT), \n",
    "    label=(\"label\", LABEL),\n",
    ")\n",
    "from IPython.core.debugger import set_trace\n",
    "dataset = data.TabularDataset(\n",
    "    path=json_file,\n",
    "    format=\"json\",\n",
    "    fields=fields,\n",
    ") \n",
    "#help(dataset)\n",
    "training_data, test_data, validation_data = dataset.split(\n",
    "    split_ratio=[0.7, 0.2, 0.1],\n",
    "    random_state=random.seed(SEED)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "69"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length (Training Data): 700\n",
      "Length (Test Data): 100\n",
      "Length (Validation Data): 200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length (Training Data): {len(training_data)}\")\n",
    "print(f\"Length (Test Data): {len(test_data)}\")\n",
    "print(f\"Length (Validation Data): {len(validation_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "70"
    }
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(training_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "71"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 1828\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "[('the', 300), ('and', 201), ('I', 195), ('is', 175), ('a', 143), ('it', 141), ('to', 136), ('phone', 105), ('this', 100), ('my', 86)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "print(TEXT.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "75"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "training_iterator, validation_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (training_data, validation_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "77"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "78"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "79"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 274,705 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "81"
    }
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "80"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "82"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "84"
    }
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    training_loss, training_acc = train(model, training_iterator, optimizer, criterion)\n",
    "    validation_loss, validation_acc = evaluate(model, validation_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\ttraining Loss: {training_loss:.3f} | training Acc: {training_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {validation_loss:.3f} |  Val. Acc: {validation_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
