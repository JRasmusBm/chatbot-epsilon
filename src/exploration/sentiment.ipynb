{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JRasmusBm/chatbot-epsilon/blob/master/Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KeH8Jy3iGkzs"
   },
   "source": [
    "# This is the file in which we perform training of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BbNL54YkGZ5G"
   },
   "outputs": [],
   "source": [
    "def perform_training():\n",
    "  for i in range(100):\n",
    "    print(f\"Progress: {i} / 100...\")\n",
    "  print(f\"Training Complete!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDQaPjEiHmvS"
   },
   "outputs": [],
   "source": [
    "perform_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWjzaSJz2rZp"
   },
   "source": [
    "#Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xZh6wQA2vGf"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXm_r12d2wCw"
   },
   "outputs": [],
   "source": [
    "file_name = \"amazon_cells_labelled.txt\"\n",
    "uploaded[file_name].decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GIhLVvpw3HiD"
   },
   "source": [
    "#Import code (from TA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AAbfYsRd3VaL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKF-Qfr63DG3"
   },
   "outputs": [],
   "source": [
    "def preprocess_pandas(data, columns):\n",
    "    df_ = pd.DataFrame(columns=columns)\n",
    "    data['Sentence'] = data['Sentence'].str.lower()\n",
    "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
    "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
    "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
    "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
    "    for index, row in data.iterrows():\n",
    "        word_tokens = word_tokenize(row['Sentence'])\n",
    "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
    "        df_ = df_.append({\n",
    "            \"index\": row['index'],\n",
    "            \"Class\": row['Class'],\n",
    "            \"Sentence\": \" \".join(filtered_sent[0:])\n",
    "        }, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# If this is the primary file that is executed (ie not an import of another file)\n",
    "if __name__ == \"__main__\":\n",
    "    # get data, pre-process and split\n",
    "    data = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
    "    data.columns = ['Sentence', 'Class']\n",
    "    data['index'] = data.index                                          # add new column index\n",
    "    columns = ['index', 'Class', 'Sentence']\n",
    "    data = preprocess_pandas(data, columns)                             # pre-process\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "        data['Sentence'].values.astype('U'),\n",
    "        data['Class'].values.astype('int32'),\n",
    "        test_size=0.10,\n",
    "        random_state=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "    word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "    training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "    training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "    vocab_size = len(word_vectorizer.vocabulary_)\n",
    "    validation_data = word_vectorizer.transform(validation_data)\n",
    "    validation_data = validation_data.todense()\n",
    "    train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "    train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "    validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "    validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhYlj6cM5YQD"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ee0vwQO5Zby"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Trainer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
