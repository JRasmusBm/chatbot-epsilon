{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JRasmusBm/chatbot-epsilon/blob/master/Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KeH8Jy3iGkzs"
   },
   "source": [
    "# This is the file in which we perform training of the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWjzaSJz2rZp"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xZh6wQA2vGf"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n",
    "#file_name = \"amazon_cells_labelled.txt\"\n",
    "#uploaded[file_name].decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXm_r12d2wCw"
   },
   "source": [
    "## Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../../data\"\n",
    "file_name = f\"{data_folder}/amazon_cells_labelled.txt\"\n",
    "json_file = f\"{data_folder}/amazon_cells_labelled.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GIhLVvpw3HiD"
   },
   "source": [
    "# Import code (from TA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pprint import pprint\n",
    "#from IPython.core.debugger import set_trace\n",
    "#import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AAbfYsRd3VaL"
   },
   "outputs": [],
   "source": [
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#import torch.nn.functional as F\n",
    "#from torch.utils.data import TensorDataset, DataLoader\n",
    "#import numpy as np\n",
    "#from matplotlib import pyplot\n",
    "#import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk import word_tokenize\n",
    "#from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKF-Qfr63DG3"
   },
   "outputs": [],
   "source": [
    "#def preprocess_pandas(data, columns):\n",
    "#    df_ = pd.DataFrame(columns=columns)\n",
    "#    data['Sentence'] = data['Sentence'].str.lower()\n",
    "#    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
    "#    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
    "#    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
    "#    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
    "#    for index, row in data.iterrows():\n",
    "#        word_tokens = word_tokenize(row['Sentence'])\n",
    "#        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
    "#        df_ = df_.append({\n",
    "#            \"index\": row['index'],\n",
    "#            \"Class\": row['Class'],\n",
    "#            \"Sentence\": \" \".join(filtered_sent[0:])\n",
    "#        }, ignore_index=True)\n",
    "#    return data\n",
    "#\n",
    "## If this is the primary file that is executed (ie not an import of another file)\n",
    "#if __name__ == \"__main__\":\n",
    "#    # get data, pre-process and split\n",
    "#    data = pd.read_csv(file_name, delimiter='\\t', header=None)\n",
    "#    data.columns = ['Sentence', 'Class']\n",
    "#    data['index'] = data.index                                          # add new column index\n",
    "#    columns = ['index', 'Class', 'Sentence']\n",
    "#    data = preprocess_pandas(data, columns)                             # pre-process\n",
    "#    training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "#        data['Sentence'].values.astype('U'),\n",
    "#        data['Class'].values.astype('int32'),\n",
    "#        test_size=0.10,\n",
    "#        random_state=0,\n",
    "#        shuffle=True\n",
    "#    )\n",
    "#\n",
    "#    # vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "#    word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "#    training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "#    training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "#    vocab_size = len(word_vectorizer.vocabulary_)\n",
    "#    validation_data = word_vectorizer.transform(validation_data)\n",
    "#    validation_data = validation_data.todense()\n",
    "#    train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "#    train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "#    validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "#    validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(train_x_tensor[0]))\n",
    "#print(vocab_size)\n",
    "##pprint(list(zip(train_x_tensor, train_y_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhYlj6cM5YQD"
   },
   "outputs": [],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ee0vwQO5Zby"
   },
   "outputs": [],
   "source": [
    "#words = [ \n",
    "#    word \n",
    "#    for line in data[\"Sentence\"] \n",
    "#    for word in line.split(\" \") if word != \"\"\n",
    "#]\n",
    "#word_counts = {}\n",
    "#for line in data[\"Sentence\"]:\n",
    "#    for word in line.split(\" \") :\n",
    "#        if word == \"\":\n",
    "#            continue\n",
    "#        if word in word_counts:\n",
    "#            word_counts[word] += 1\n",
    "#        else:\n",
    "#            word_counts[word] = 1\n",
    "#    \n",
    "#unique_words = set(words)\n",
    "#prevalent_words = sorted(word_counts.items(), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create lists of labels and sentences. The indices in the one correspond to those in the other. Due to restrictions in torchtext, write it as json to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_name) as f:\n",
    "    contents = f.read()\n",
    "labels = []\n",
    "sentences = []\n",
    "for line in (l for l in contents.split(\"\\n\") if l):\n",
    "    labels.append(int(line[-1]))\n",
    "    sentences.append(str.strip(line[:-1]).replace(\n",
    "        \".\", \"\").replace(\n",
    "        \"!\", \"\").replace(\n",
    "        \"?\", \"\").replace(\n",
    "        \",\", \"\").split(\" \"))\n",
    "data_json = [\n",
    "    dict(label=label, sentence=sentence)\n",
    "    for label, sentence in zip(labels, sentences)\n",
    "]\n",
    "with open(json_file, \"w\") as f:\n",
    "    text = \"\\n\".join(json.dumps(line) for line in data_json)\n",
    "    f.write(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the lists of labels into a torchtext dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length (Training Data): 700\n",
      "Length (Test Data): 100\n",
      "Length (Validation Data): 200\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.utils.data.dataset import random_split\n",
    "SEED = 1234\n",
    "pairs = list(zip(sentences, labels))\n",
    "fields=dict(\n",
    "    sentence=(\"sentence\", data.Field(sequential=True)), \n",
    "    label=(\"label\", data.Field(sequential=False)),\n",
    ")\n",
    "from IPython.core.debugger import set_trace\n",
    "dataset = data.TabularDataset(\n",
    "    path=json_file,\n",
    "    format=\"json\",\n",
    "    fields=fields,\n",
    ") \n",
    "#help(dataset)\n",
    "training_data, test_data, validation_data = dataset.split(\n",
    "    split_ratio=[0.7, 0.2, 0.1],\n",
    "    random_state=random.seed(SEED)\n",
    ")\n",
    "\n",
    "print(f\"Length (Training Data): {len(training_data)}\")\n",
    "print(f\"Length (Test Data): {len(test_data)}\")\n",
    "print(f\"Length (Validation Data): {len(validation_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-22738ed5d6e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        output, hidden = self.rnn(text, 1)\n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_dim=vocab_size, hidden_dim=256, output_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"There are {count_parameters(model):,} trainable parameters in the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, sentences, labels, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(sentence).squeeze(1)\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = binary_accuracy(predictions, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval() \n",
    "    with torch.no_grad(): \n",
    "        for sentence, label in iterator: \n",
    "            predictions = model(sentence).squeeze(1) \n",
    "            loss = criterion(predictions, label) \n",
    "            acc = binary_accuracy(predictions, label) \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(model, train_x_tensor, train_y_tensor, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_x_tensor, valid_y_tensor, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Trainer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
