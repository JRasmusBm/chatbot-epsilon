{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/JRasmusBm/chatbot-\n",
    "epsilon/blob/master/Trainer.ipynb\" target=\"_parent\"><img\n",
    "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In\n",
    "Colab\"/></a>\n",
    "\n",
    "# This is the file in which we perform training of the NN\n",
    "\n",
    "# Load\n",
    "Data\n",
    "\n",
    "## In Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n",
    "#file_name = \"amazon_cells_labelled.txt\"\n",
    "#uploaded[file_name].decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "data_folder = \"../../data\"\n",
    "trained_models_folder = \"../../trained_models\"\n",
    "file_name = f\"{data_folder}/amazon_cells_labelled.txt\"\n",
    "json_file = f\"{data_folder}/amazon_cells_labelled.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import code (from TA)\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data\n",
    "\n",
    "First, we create lists of labels and sentences. The indices in\n",
    "the one\n",
    "correspond to those in the other. Due to restrictions in torchtext,\n",
    "write it as\n",
    "json to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "with open(file_name) as f:\n",
    "    contents = f.read()\n",
    "labels = []\n",
    "sentences = []\n",
    "for line in (l for l in contents.split(\"\\n\") if l):\n",
    "    labels.append(int(line[-1]))\n",
    "    sentence = str.strip(line[:-1])\n",
    "    while len(sentence.split(\" \")) < 5:\n",
    "        sentence += \" a\"\n",
    "    sentences.append(sentence)\n",
    "data_json = [\n",
    "    dict(label=label, text=text) for label, text in zip(labels, sentences)\n",
    "]\n",
    "with open(json_file, \"w\") as f:\n",
    "    text = \"\\n\".join(json.dumps(line) for line in data_json)\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "with open(json_file) as f:\n",
    "    json_written = [json.loads(line) for line in f.read().split(\"\\n\")]\n",
    "    for line in json_written:\n",
    "        if line[\"label\"] not in [0, 1]:\n",
    "            print(line)\n",
    "        if len(line[\"text\"].split(\" \")) < 5:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Torchtext Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "def generate_bigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(\" \".join(n_gram))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# help(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "init_token_index = tokenizer.cls_token_id\n",
    "end_of_string_token_index = tokenizer.sep_token_id\n",
    "padding_token_index = tokenizer.pad_token_id\n",
    "unknown_token_index = tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes[\"bert-base-uncased\"]\n",
    "\n",
    "TEXT = data.Field(\n",
    "    batch_first=True,\n",
    "    use_vocab=False,\n",
    "    preprocessing=tokenizer.convert_tokens_to_ids,\n",
    "    init_token=init_token_index,\n",
    "    eos_token=end_of_string_token_index,\n",
    "    pad_token=padding_token_index,\n",
    "    unk_token=unknown_token_index,\n",
    ")\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "fields = dict(text=(\"text\", TEXT), label=(\"label\", LABEL),)\n",
    "\n",
    "dataset = data.TabularDataset(path=json_file, format=\"json\", fields=fields,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "training_data, test_data, validation_data = dataset.split(\n",
    "    split_ratio=[0.7, 0.2, 0.1], random_state=random.seed(SEED)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = (tokens[: max_input_length - 2],)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length (Training Data): 700\n",
      "Length (Test Data): 100\n",
      "Length (Validation Data): 200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length (Training Data): {len(training_data)}\")\n",
    "print(f\"Length (Test Data): {len(test_data)}\")\n",
    "print(f\"Length (Validation Data): {len(validation_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "LABEL.build_vocab(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 30522\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(tokenizer.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "training_iterator, validation_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (training_data, validation_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(\n",
    "        self, bert, hidden_dim, output_dim, n_layers, bidirectional, dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()[\"hidden_size\"]\n",
    "        self.rnn = nn.GRU(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "            dropout=0 if n_layers < 2 else dropout,\n",
    "        )\n",
    "        self.out = nn.Linear(\n",
    "            hidden_dim * 2 if bidirectional else hidden_dim, output_dim\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        with torch.no_grad():\n",
    "            embedded = bert(text)[0]\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(\n",
    "                torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "            )\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1, :, :])\n",
    "        output = self.out(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "    HIDDEN_DIM,\n",
    "    OUTPUT_DIM,\n",
    "    N_LAYERS,\n",
    "    BIDIRECTIONAL,\n",
    "    DROPOUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 112,241,409 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"bert\"):\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "25"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n",
      "rnn.bias_ih_l0\n",
      "rnn.bias_hh_l0\n",
      "rnn.weight_ih_l0_reverse\n",
      "rnn.weight_hh_l0_reverse\n",
      "rnn.bias_ih_l0_reverse\n",
      "rnn.bias_hh_l0_reverse\n",
      "rnn.weight_ih_l1\n",
      "rnn.weight_hh_l1\n",
      "rnn.bias_ih_l1\n",
      "rnn.bias_hh_l1\n",
      "rnn.weight_ih_l1_reverse\n",
      "rnn.weight_hh_l1_reverse\n",
      "rnn.bias_ih_l1_reverse\n",
      "rnn.bias_hh_l1_reverse\n",
      "out.weight\n",
      "out.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  if param.requires_grad:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "81"
    }
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "80"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "82"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "84"
    }
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 33s\n",
      "\ttraining Loss: 0.754 | training Acc: 51.87%\n",
      "\t Val. Loss: 0.700 |  Val. Acc: 44.14%\n",
      "Epoch: 02 | Epoch Time: 0m 33s\n",
      "\ttraining Loss: 0.636 | training Acc: 66.07%\n",
      "\t Val. Loss: 0.648 |  Val. Acc: 67.19%\n",
      "Epoch: 03 | Epoch Time: 0m 37s\n",
      "\ttraining Loss: 0.553 | training Acc: 72.91%\n",
      "\t Val. Loss: 0.737 |  Val. Acc: 59.38%\n",
      "Epoch: 04 | Epoch Time: 0m 39s\n",
      "\ttraining Loss: 0.631 | training Acc: 68.02%\n",
      "\t Val. Loss: 0.879 |  Val. Acc: 43.36%\n",
      "Epoch: 05 | Epoch Time: 0m 34s\n",
      "\ttraining Loss: 0.656 | training Acc: 61.92%\n",
      "\t Val. Loss: 0.655 |  Val. Acc: 64.45%\n",
      "Epoch: 06 | Epoch Time: 0m 34s\n",
      "\ttraining Loss: 0.524 | training Acc: 73.48%\n",
      "\t Val. Loss: 0.497 |  Val. Acc: 76.56%\n",
      "Epoch: 07 | Epoch Time: 0m 33s\n",
      "\ttraining Loss: 0.496 | training Acc: 73.87%\n",
      "\t Val. Loss: 0.532 |  Val. Acc: 73.05%\n",
      "Epoch: 08 | Epoch Time: 0m 33s\n",
      "\ttraining Loss: 0.465 | training Acc: 75.34%\n",
      "\t Val. Loss: 0.518 |  Val. Acc: 71.88%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    training_loss, training_acc = train(model, training_iterator, optimizer, criterion)\n",
    "    validation_loss, validation_acc = evaluate(model, validation_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(model.state_dict(), f\"{trained_models_folder}/bert_10_given_dataset.pt\")\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\ttraining Loss: {training_loss:.3f} | training Acc: {training_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {validation_loss:.3f} |  Val. Acc: {validation_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
